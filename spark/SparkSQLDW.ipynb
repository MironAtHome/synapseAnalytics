{
  "metadata": {
    "language_info": {
      "name": "scala"
    },
    "sessionKeepAliveTimeout": 30
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "java.lang.ClassNotFoundException: com.databricks.spark.sqldw.DefaultSource",
          "traceback": [
            "Error : java.lang.ClassNotFoundException: com.databricks.spark.sqldw.DefaultSource",
            "  at scala.reflect.internal.util.AbstractFileClassLoader.findClass(AbstractFileClassLoader.scala:62)\n",
            "  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n",
            "  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n",
            "  at java.lang.Class.forName0(Native Method)\n",
            "  at java.lang.Class.forName(Class.java:264)\n",
            "  ... 52 elided\n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "Class.forName(\"com.databricks.spark.sqldw.DefaultSource\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "execution_count": 4,
          "output_type": "execute_result",
          "data": {
            "text/plain": "res3: Class[_] = class com.microsoft.sqlserver.jdbc.SQLServerDriver"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "Class.forName(\"com.microsoft.sqlserver.jdbc.SQLServerDriver\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {},
      "source": [
        "spark.conf.set(\n",
        "  \"fs.azure.account.key.waginput.blob.core.windows.net\",\n",
        "  \"TBwLoPOLim87APX5grtZzWy8Td9h69F/BJgDxuiQyEP480Cs5zyOa2bUeVVRfUnCALOug3aA2Wb4cj8aIiqGEw==\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [
        {
          "execution_count": 9,
          "output_type": "execute_result",
          "data": {
            "text/plain": "jdbcHostname: String = synapadls.sql.azuresynapse.net\njdbcPort: Int = 1433\njdbcDatabase: String = dwpool\njdbcurl: String = jdbc:sqlserver://synapadls.sql.azuresynapse.net:1433;database=dwpool;user=sqladmin@synapadls;password=Azure!2345678;encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "val jdbcHostname = \"synapadls.sql.azuresynapse.net\"\n",
        "val jdbcPort = 1433\n",
        "val jdbcDatabase = \"dwpool\"\n",
        "\n",
        "// Create the JDBC URL without passing in the user and password parameters.\n",
        "//val jdbcUrl = s\"jdbc:sqlserver://${jdbcHostname}:${jdbcPort};database=${jdbcDatabase}\"\n",
        "val jdbcurl = s\"jdbc:sqlserver://synapadls.sql.azuresynapse.net:1433;database=dwpool;user=sqladmin@synapadls;password=Azure!2345678;encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "execution_count": 10,
          "output_type": "execute_result",
          "data": {
            "text/plain": "import java.util.Properties\nconnectionProperties: java.util.Properties = {}\nres15: Object = null\nres16: Object = null"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Create a Properties() object to hold the parameters.\n",
        "import java.util.Properties\n",
        "val connectionProperties = new Properties()\n",
        "\n",
        "connectionProperties.put(\"user\", s\"sqladmin\")\n",
        "connectionProperties.put(\"password\", s\"Azure!2345678\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "driverClass: String = com.microsoft.sqlserver.jdbc.SQLServerDriver\nres17: Object = null"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "val driverClass = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
        "connectionProperties.setProperty(\"Driver\", driverClass)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [
        {
          "execution_count": 12,
          "output_type": "execute_result",
          "data": {
            "text/plain": "taxidata_table: org.apache.spark.sql.DataFrame = [id: bigint, fare_amount: double ... 6 more fields]"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "val taxidata_table = spark.read.jdbc(jdbcUrl, \"taxidata\", connectionProperties)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "root\n |-- id: long (nullable = true)\n |-- fare_amount: double (nullable = true)\n |-- pickup_datetime: timestamp (nullable = true)\n |-- pickup_longitude: double (nullable = true)\n |-- pickup_latitude: double (nullable = true)\n |-- dropoff_longitude: double (nullable = true)\n |-- dropoff_latitude: double (nullable = true)\n |-- passenger_count: integer (nullable = true)"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "taxidata_table.printSchema"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [
        {
          "execution_count": 14,
          "output_type": "execute_result",
          "data": {
            "text/plain": "res19: Array[org.apache.spark.sql.Row] = Array([1887601,8.5,2009-02-04 07:37:00.0,0.0,0.0,0.0,0.0,1], [1197002,7.3,2009-04-14 08:55:20.0,0.0,0.0,0.0,0.0,1], [379023,10.5,2010-03-07 12:38:31.0,0.0,0.0,0.0,0.0,1], [6354964,5.3,2012-03-02 00:44:54.0,0.0,0.0,0.0,0.0,1], [376325,4.9,2010-09-07 09:43:53.0,0.0,0.0,0.0,0.0,1], [1444446,6.5,2011-05-20 02:12:00.0,0.0,0.0,0.0,0.0,1], [2600347,6.9,2010-08-30 09:48:39.0,0.0,0.0,0.0,0.0,1], [2952668,8.9,2012-08-24 10:38:00.0,0.0,0.0,0.0,0.0,1], [3847329,6.5,2015-01-05 07:03:37.0,0.0,0.0,0.0,0.0,1], [3773050,6.5,2010-12-11 12:02:31.0,0.0,0.0,0.0,0.0,1])"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "taxidata_table.head(10)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "//write\n",
        "//https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/sql-databases\n",
        "spark.table(\"taxidata_table\")\n",
        "     .write\n",
        "     .mode(SaveMode.Append)\n",
        "     .jdbc(jdbcUrl, \"taxidata2\", connectionProperties)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "// Load data from a SQL DW query.\n",
        "val df: DataFrame = spark.read\n",
        "  .format(\"com.databricks.spark.sqldw\")\n",
        "  .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\")\n",
        "  .option(\"tempDir\", \"wasbs://<your-container-name>@<your-storage-account-name>.blob.core.windows.net/<your-directory-name>\")\n",
        "  .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n",
        "  .option(\"query\", \"select x, count(*) as cnt from my_table_in_dw group by x\")\n",
        "  .load()"
      ],
      "attachments": {}
    }
  ]
}
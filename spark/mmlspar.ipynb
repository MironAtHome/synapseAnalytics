{
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {},
      "source": [
        "import mmlspark"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {},
      "source": [
        "triazines = spark.read.format(\"libsvm\")\\\n",
        "    .load(\"wasbs://publicwasb@mmlspark.blob.core.windows.net/triazines.scale.svmlight\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "records read: 105\nSchema: \nroot\n |-- label: double (nullable = true)\n |-- features: vector (nullable = true)\n\n   label                                           features\n0  0.809  (-0.6, -0.3325, -0.3325, -1.0, -1.0, -1.0, -1....\n1  0.602  (-0.6, 0.0, 0.0, -1.0, -0.3325, -1.0, -1.0, 0....\n2  0.442  (-0.6, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1....\n3  0.718  (-0.6, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1....\n4  0.697  (-0.6, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1....\n5  0.757  (0.2, -0.6675, -1.0, -1.0, -1.0, 0.0, -1.0, 0....\n6  0.900  (0.2, -0.6675, -1.0, -1.0, -1.0, 0.0, -1.0, 0....\n7  0.564  (-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1....\n8  0.772  (0.2, -0.6675, -1.0, -1.0, -1.0, 0.0, -1.0, 0....\n9  0.801  (0.2, -0.6675, -1.0, -1.0, -1.0, 0.0, -1.0, 0....\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:2103: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n  Unsupported type in conversion to Arrow: VectorUDT\nAttempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true."
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# print some basic info\n",
        "print(\"records read: \" + str(triazines.count()))\n",
        "print(\"Schema: \")\n",
        "triazines.printSchema()\n",
        "triazines.limit(10).toPandas()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {},
      "source": [
        "train, test = triazines.randomSplit([0.85, 0.15], seed=1)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {},
      "source": [
        "from mmlspark.lightgbm import LightGBMRegressor\n",
        "model = LightGBMRegressor(objective='quantile',\n",
        "                          alpha=0.2,\n",
        "                          learningRate=0.3,\n",
        "                          numLeaves=31).fit(train)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {},
      "source": [
        "from mmlspark.lightgbm import LightGBMRegressionModel\n",
        "model.saveNativeModel(\"mymodel\")\n",
        "model = LightGBMRegressionModel.loadNativeModelFromFile(\"mymodel\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "[18.0, 4.0, 8.0, 0.0, 16.0, 16.0, 0.0, 3.0, 2.0, 0.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0, 27.0, 27.0, 18.0, 28.0, 28.0, 0.0, 10.0, 0.0, 4.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10.0, 0.0, 0.0, 0.0]"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "print(model.getFeatureImportances())"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "label    ...     prediction\n0  0.258    ...       0.414115\n1  0.427    ...       0.539532\n2  0.550    ...       0.537624\n3  0.614    ...       0.640256\n4  0.631    ...       0.422801\n5  0.637    ...       0.521593\n6  0.641    ...       0.585361\n7  0.678    ...       0.585361\n8  0.788    ...       0.726604\n9  0.801    ...       0.634850\n\n[10 rows x 3 columns]"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "scoredData = model.transform(test)\n",
        "scoredData.limit(10).toPandas()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "mean_squared_error         ...           mean_absolute_error\n0            0.014862         ...                      0.107673\n\n[1 rows x 4 columns]\n/home/trusted-service-user/cluster-env/env/lib/python3.6/site-packages/pyarrow/__init__.py:152: UserWarning: pyarrow.open_stream is deprecated, please use pyarrow.ipc.open_stream\n  warnings.warn(\"pyarrow.open_stream is deprecated, please use \""
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "from mmlspark.train import ComputeModelStatistics\n",
        "metrics = ComputeModelStatistics(evaluationMetric='regression',\n",
        "                                 labelCol='label',\n",
        "                                 scoresCol='prediction') \\\n",
        "            .transform(scoredData)\n",
        "metrics.toPandas()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.ml import Transformer, Estimator, Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from mmlspark.downloader import ModelDownloader\n",
        "import os, sys, time"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {},
      "source": [
        "model = ModelDownloader(spark, \"/models/\").downloadByName(\"ResNet50\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "root\n |-- filename: string (nullable = true)\n |-- image: binary (nullable = true)\n |-- labels: double (nullable = true)"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Load the images\n",
        "# use flowers_and_labels.parquet on larger cluster in order to get better results\n",
        "imagesWithLabels = spark.read.parquet(\"wasbs://publicwasb@mmlspark.blob.core.windows.net/flowers_and_labels2.parquet\") \\\n",
        "    .withColumnRenamed(\"bytes\",\"image\").sample(.1)\n",
        "\n",
        "imagesWithLabels.printSchema()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {},
      "source": [
        "from mmlspark.opencv import ImageTransformer\n",
        "from mmlspark.image import UnrollImage, ImageFeaturizer\n",
        "from mmlspark.stages import *\n",
        "\n",
        "# Make some featurizers\n",
        "it = ImageTransformer()\\\n",
        "    .setOutputCol(\"scaled\")\\\n",
        "    .resize(height = 60, width = 60)\n",
        "\n",
        "ur = UnrollImage()\\\n",
        "    .setInputCol(\"scaled\")\\\n",
        "    .setOutputCol(\"features\")\n",
        "    \n",
        "dc1 = DropColumns().setCols([\"scaled\", \"image\"])\n",
        "\n",
        "lr1 = LogisticRegression().setMaxIter(8).setFeaturesCol(\"features\").setLabelCol(\"labels\")\n",
        "\n",
        "dc2 = DropColumns().setCols([\"features\"])\n",
        "\n",
        "basicModel = Pipeline(stages=[it, ur, dc1, lr1, dc2])"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {},
      "source": [
        "resnet = ImageFeaturizer()\\\n",
        "    .setInputCol(\"image\")\\\n",
        "    .setOutputCol(\"features\")\\\n",
        "    .setModelLocation(model.uri)\\\n",
        "    .setLayerNames(model.layerNames)\\\n",
        "    .setCutOutputLayers(1)\n",
        "    \n",
        "dc3 = DropColumns().setCols([\"image\"])\n",
        "    \n",
        "lr2 = LogisticRegression().setMaxIter(8).setFeaturesCol(\"features\").setLabelCol(\"labels\")\n",
        "\n",
        "dc4 = DropColumns().setCols([\"features\"])\n",
        "\n",
        "deepModel = Pipeline(stages=[resnet, dc3, lr2, dc4])"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {},
      "source": [
        "def timedExperiment(model, train, test):\n",
        "  start = time.time()\n",
        "  result =  model.fit(train).transform(test).toPandas()\n",
        "  print(\"Experiment took {}s\".format(time.time() - start))\n",
        "  return result"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "(623, 164)"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "train, test = imagesWithLabels.randomSplit([.8,.2])\n",
        "train.count(), test.count()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 20,
          "data": {
            "text/plain": "Experiment took 93.33845472335815s"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "basicResults = timedExperiment(basicModel, train, test)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 21,
          "data": {
            "text/plain": "Experiment took 179.01435899734497s"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "deepResults = timedExperiment(deepModel, train, test)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "status": "error",
          "execution_count": 23,
          "data": null,
          "ename": "Exception",
          "evalue": "Not support to display this type of data: <class 'NoneType'>",
          "traceback": [
            "Exception : Not support to display this type of data: <class 'NoneType'>",
            "Traceback (most recent call last):\n",
            "  File \"<stdin>\", line 334, in display\n",
            "Exception: Not support to display this type of data: <class 'NoneType'>\n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "def evaluate(results, name):\n",
        "    y, y_hat = results[\"labels\"],results[\"prediction\"]\n",
        "    y = [int(l) for l in y]\n",
        "\n",
        "    accuracy = np.mean([1. if pred==true else 0. for (pred,true) in zip(y_hat,y)])\n",
        "    cm = confusion_matrix(y, y_hat)\n",
        "    cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    plt.text(40, 10,\"$Accuracy$ $=$ ${}\\%$\".format(round(accuracy*100,1)),fontsize=14)\n",
        "    plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"$Predicted$ $label$\", fontsize=18)\n",
        "    plt.ylabel(\"$True$ $Label$\", fontsize=18)\n",
        "    plt.title(\"$Normalized$ $CM$ $for$ ${}$\".format(name))\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,2,1)\n",
        "evaluate(deepResults,\"CNTKModel + LR\")\n",
        "plt.subplot(1,2,2)\n",
        "evaluate(basicResults,\"LR\")\n",
        "# Note that on the larger dataset the accuracy will bump up from 44% to >90%\n",
        "display(plt.show())"
      ],
      "attachments": {}
    }
  ]
}